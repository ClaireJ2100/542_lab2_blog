{
  "hash": "d1fe0736fa039c04be198dd93ee59f33",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Mastering Regression: A Dive into Ridge and Logistic Regression\"\nauthor: \"Zhengling Jiang\"\ndate: \"2025-01-18\"\ncategories: [Machine Learning, Data Science, Regression, Python]\nimage: \"image.jpg\"\njupyter: python3\nformat:\n  html:\n    mathjax: true\n---\n\n\n\n## Introduction\nRegression is a supervised machine learning technique used to model relationships between variables and make predictions. Among the many regression techniques, ridge regression and logistic regression stand out for their strengths and applications. In this blog, we will dive into these methods, explain their concepts, and guide you through practical implementations via Python. \n\n## What is Regression?\nRegression is a statistical method for understanding the relationships between features and outcome. Unlike simple linear regression or multiple linear regression, advanced methods like ridge regression and logistic regression can address specific challenges like multicollinearity and classification. \n\n## Ridge Regression\nRidge Regression is a linear regression model with a regularization alpha to reduce overfitting. This method adds the squared magnitude of the coefficients as a penalty, it prevents excessively large coefficients that can improve predictive performance. Ridge Regression minimizes this expression:\n$$RSS_{Ridge} = (\\mathbf{Y} - \\mathbf{X} {\\beta})^T (\\mathbf{Y} - \\mathbf{X} {\\beta}) + \\lambda {\\beta}^T {\\beta} = \\|\\mathbf{Y} - \\mathbf{X}{\\beta}\\|^2 + \\lambda \\|{\\beta}\\|^2 $$\n\nwhere $\\lambda$ is the penalty term. If we increase the hyperparameter alpha of Ridge, it is likely to decrease model complexity. If we set the alpha to zero, it is the same as LinearRegression. \n\nCode example:\n\n::: {#bc2c9025 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Load dataset\nfrom sklearn.datasets import load_diabetes\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Ridge regression model\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = ridge.predict(X_test)\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Ridge Regression Predictions vs Actuals\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 3077.41593882723\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=593 height=449}\n:::\n:::\n\n\nThe MSE of 3077.42 indicates the average squared difference between predicted and actual values, with lower values reflecting better performance. This suggests the model performs reasonably well but could be improved through further improvements.\n\nThe scatter plot compares the actual values with the predicted values generated by the Ridge Regression model. Most points lie on a 45-degree diagonal line, which indicating this model's predictions are good.\n\n## Logistic Regression\nLogistic regression is a classification algorithm that predicts probabilities of categorical outcomes. The logit function maps predictions to probabilities, making it ideal for binary classification tasks. The logit function can be expressed as:\n$\\mbox{logit}(p_i)= \\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_{i}.$\nIn this expression, logit(pi) is the dependent variable and X is the independent variable. Logit odds can be difficult to interpret, thus, we have to transform back our estimated coefficient to the original scale. It is common to exponentiate the coefficient into an odds ratio. The general interpretation is: for each unit increase in $X_i$, $Y_i$ is $e^{\\hat{\\beta}_1}$ times more likely to be 1 than to be 0. \n\nCode example:\n\n::: {#bc001221 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n# Load dataset\nfrom sklearn.datasets import load_iris\nX, y = load_iris(return_X_y=True)\nX, y = X[y != 2], y[y != 2]  # Binary classification\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Logistic Regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = logreg.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\ndisp.plot(cmap='Blues', values_format='d')\n\nplt.title(\"Confusion Matrix for Logistic Regression\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=496 height=449}\n:::\n:::\n\n\nThe accuracy of 1.00 shows perfect classification on the test dataset, likely due to the dataset's simplicity and separability.\n\nThe confusion matrix shows the counts of true positive, true negative, false positive, and false negative predictions.\n\n## Comparing Ridge and Logistic Regression\nWhile Ridge Regression is used for continuous outcomes, Logistic Regression addresses binary and multi-class classification tasks. Here are some insights into their comparison:\n\n1. **Mathematical foundation:** Ridge regression minimizes the residual sum of squares with an L2 penalty, while logistic regression optimizes the log-likelihood function with regularization.\n\n2. **Scaling:** both ridge regression and logistic regression typically benefit from scaling the input data, especially when using numerical features of varying ranges.\n\n3. **Output interpretability:** Ridge regression provides straightforward coefficient estimates for continuous predictions, whereas logistic regression outputs probabilities for different classes.\n\n| Aspect                 | Ridge Regression         | Logistic Regression        |\n|------------------------|--------------------------|----------------------------|\n| Task Type              | Regression (continuous) | Classification (binary/multi-class) |\n| Regularization         | L2                      | L1, L2, or Elastic Net     |\n| Output                 | Continuous values       | Probabilities              |\n\n## Real-world Applications of Regression\nRidge regression helps us find out the relationships in data, and answer questions like: whatâ€™s the relationship between the amount of time you study and the exam scores. Many real-world cases could be solved by ridge regression:\n\n1. **Real Estate:** Estimating house prices by considering factors like location, size, and amenities.\n\n2. **Finance:** Forecasting stock prices and portfolio returns.\n\n3. **Marketing:** Predicting sales performance based on advertisement spend.\n\nIn real life, logistic regression is frequently used to forecast binary or multi-class outcomes, such as success/failure, win/lose. Some of the real-world cases are:\n\n\n1. **Healthcare:** Predicting the likelihood of heart disease.\n\n2. **Finance:** Detecting fraudulent transactions based on transaction patterns.\n\n3. **Churn prediction:** Predicting customer churn for subscription-based services.\n\n## Tips for Effective Regression Modeling\n- Improve the data quality by removing outliers, and handling missing values.\n- Choosing the optimal hyperparameters.\n- Detecting multicollinearity in data by using metrics like Variance Inflation Factor (VIF).\n- Feature Selection: Use techniques like PCA dimensionality reduction.\n- Interpret Coefficients: Understand the impact of each feature on predictions.\n\n## Conclusion\nRegression techniques like ridge and logistic regression are essential tools for data scientists. By mastering these models, you can tackle a wide range of prediction and classification problems in real life. Ridge regression helps in predicting continuous variables while handling overfitting through regularization. On the other hand, logistic regression helps to make decisions in binary and multi-class tasks. \n\n## References\n- [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}