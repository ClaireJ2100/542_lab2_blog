---
title: "Mastering Regression: A Dive into Ridge and Logistic Regression"
author: "Zhengling Jiang"
date: "2025-01-18"
categories: [Machine Learning, Data Science, Regression, Python]
image: "image.jpg"
jupyter: python3
format:
  html:
    mathjax: true
---


## Introduction
Regression is a supervised machine learning technique used to model relationships between variables and make predictions. Among the many regression techniques, ridge regression and logistic regression stand out for their strengths and applications. In this blog, we will dive into these methods, explain their concepts, and guide you through practical implementations via Python. 

## What is Regression?
Regression is a statistical method for understanding the relationships between features and outcome. Unlike simple linear regression or multiple linear regression, advanced methods like ridge regression and logistic regression can address specific challenges like multicollinearity and classification. 

## Ridge Regression
Ridge Regression is a linear regression model with a regularization alpha to reduce overfitting. This method adds the squared magnitude of the coefficients as a penalty, it prevents excessively large coefficients that can improve predictive performance. Ridge Regression minimizes this expression:
$$RSS_{Ridge} = (\mathbf{Y} - \mathbf{X} {\beta})^T (\mathbf{Y} - \mathbf{X} {\beta}) + \lambda {\beta}^T {\beta} = \|\mathbf{Y} - \mathbf{X}{\beta}\|^2 + \lambda \|{\beta}\|^2 $$

where $\lambda$ is the penalty term. If we increase the hyperparameter alpha of Ridge, it is likely to decrease model complexity. If we set the alpha to zero, it is the same as LinearRegression. 
Code example:
```{python}
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load dataset
from sklearn.datasets import load_diabetes
X, y = load_diabetes(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Ridge regression model
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Predictions and evaluation
y_pred = ridge.predict(X_test)
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Ridge Regression Predictions vs Actuals")
plt.show()
```
The scatter plot compares the actual values with the predicted values generated by the Ridge Regression model. Most points lie on a 45-degree diagonal line, which indicating this model's predictions are good.

## Logistic Regression
Logistic regression is a classification algorithm that predicts probabilities of categorical outcomes. The logit function maps predictions to probabilities, making it ideal for binary classification tasks. The logit function can be expressed as:
$\mbox{logit}(p_i)= \log\left(\frac{p_i}{1 - p_i}\right) = \beta_0 + \beta_1 X_{i}.$
In this expression, logit(pi) is the dependent variable and X is the independent variable. Logit odds can be difficult to interpret, thus, we have to transform back our estimated coefficient to the original scale. It is common to exponentiate the coefficient into an odds ratio. The general interpretation is: for each unit increase in $X_i$, $Y_i$ is $e^{\hat{\beta}_1}$ times more likely to be 1 than to be 0. 

Code example:
```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
import matplotlib.pyplot as plt


# Load dataset
from sklearn.datasets import load_iris
X, y = load_iris(return_X_y=True)
X, y = X[y != 2], y[y != 2]  # Binary classification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression model
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Predictions and evaluation
y_pred = logreg.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)
disp.plot(cmap='Blues', values_format='d')

plt.title("Confusion Matrix for Logistic Regression")
plt.show()
```
The confusion matrix shows the counts of true positive, true negative, false positive, and false negative predictions.

## Comparing Ridge and Logistic Regression
While Ridge Regression is used for continuous outcomes, Logistic Regression addresses binary and multi-class classification tasks. Here are some insights into their comparison:
- **Mathematical foundation:** Ridge regression minimizes the residual sum of squares with an L2 penalty, while logistic regression optimizes the log-likelihood function with regularization.
- **Scaling:** both ridge regression and logistic regression typically benefit from scaling the input data, especially when using numerical features of varying ranges.
- **Output interpretability:** Ridge regression provides straightforward coefficient estimates for continuous predictions, whereas logistic regression outputs probabilities for different classes.

## Real-world Applications of Regression
Ridge regression helps us find out the relationships in data, and answer questions like: whatâ€™s the relationship between the amount of time you study and the exam scores. Many real-world cases could be solved by ridge regression:
- **Real Estate:** Estimating house prices by considering factors like location, size, and amenities.
- **Finance:** Forecasting stock prices and portfolio returns.
- **Marketing:** Predicting sales performance based on advertisement spend.

In real life, logistic regression is frequently used to forecast binary or multi-class outcomes, such as success/failure, win/lose. Some of the real-world cases are:
- **Healthcare:** Predicting the likelihood of heart disease.
- **Finance:** Detecting fraudulent transactions based on transaction patterns.
- **Churn prediction:** Predicting customer churn for subscription-based services.

## Tips for Effective Regression Modeling
- Improve the data quality by removing outliers, and handling missing values.
- Choosing the optimal hyperparameters.
- Detecting multicollinearity in data by using metrics like Variance Inflation Factor (VIF).
- Feature Selection: Use techniques like PCA dimensionality reduction.
- Interpret Coefficients: Understand the impact of each feature on predictions.

## Conclusion
Regression techniques like ridge and logistic regression are essential tools for data scientists. By mastering these models, you can tackle a wide range of prediction and classification problems in real life. Ridge regression helps in predicting continuous variables while handling overfitting through regularization. On the other hand, logistic regression helps to make decisions in binary and multi-class tasks. 

## References
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)