[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nCJ was here"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "",
    "text": "Regression is a supervised machine learning technique used to model relationships between variables and make predictions. Among the many regression techniques, ridge regression and logistic regression stand out for their strengths and applications. In this blog, we will dive into these methods, explain their concepts, and guide you through practical implementations via Python."
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "",
    "text": "Regression is a supervised machine learning technique used to model relationships between variables and make predictions. Among the many regression techniques, ridge regression and logistic regression stand out for their strengths and applications. In this blog, we will dive into these methods, explain their concepts, and guide you through practical implementations via Python."
  },
  {
    "objectID": "posts/post-with-code/index.html#what-is-regression",
    "href": "posts/post-with-code/index.html#what-is-regression",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "What is Regression?",
    "text": "What is Regression?\nRegression is a statistical method for understanding the relationships between features and outcome. Unlike simple linear regression or multiple linear regression, advanced methods like ridge regression and logistic regression can address specific challenges like multicollinearity and classification."
  },
  {
    "objectID": "posts/post-with-code/index.html#ridge-regression",
    "href": "posts/post-with-code/index.html#ridge-regression",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nRidge Regression is a linear regression model with a regularization alpha to reduce overfitting. This method adds the squared magnitude of the coefficients as a penalty, it prevents excessively large coefficients that can improve predictive performance. Ridge Regression minimizes this expression: \\[RSS_{Ridge} = (\\mathbf{Y} - \\mathbf{X} {\\beta})^T (\\mathbf{Y} - \\mathbf{X} {\\beta}) + \\lambda {\\beta}^T {\\beta} = \\|\\mathbf{Y} - \\mathbf{X}{\\beta}\\|^2 + \\lambda \\|{\\beta}\\|^2 \\]\nwhere \\(\\lambda\\) is the penalty term. If we increase the hyperparameter alpha of Ridge, it is likely to decrease model complexity. If we set the alpha to zero, it is the same as LinearRegression.\nCode example:\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Load dataset\nfrom sklearn.datasets import load_diabetes\nX, y = load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Ridge regression model\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = ridge.predict(X_test)\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Ridge Regression Predictions vs Actuals\")\nplt.show()\n\nMean Squared Error: 3077.41593882723\n\n\n\n\n\n\n\n\n\nThe MSE of 3077.42 indicates the average squared difference between predicted and actual values, with lower values reflecting better performance. This suggests the model performs reasonably well but could be improved through further improvements.\nThe scatter plot compares the actual values with the predicted values generated by the Ridge Regression model. Most points lie on a 45-degree diagonal line, which indicating this model’s predictions are good."
  },
  {
    "objectID": "posts/post-with-code/index.html#logistic-regression",
    "href": "posts/post-with-code/index.html#logistic-regression",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a classification algorithm that predicts probabilities of categorical outcomes. The logit function maps predictions to probabilities, making it ideal for binary classification tasks. The logit function can be expressed as: \\(\\mbox{logit}(p_i)= \\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_{i}.\\) In this expression, logit(pi) is the dependent variable and X is the independent variable. Logit odds can be difficult to interpret, thus, we have to transform back our estimated coefficient to the original scale. It is common to exponentiate the coefficient into an odds ratio. The general interpretation is: for each unit increase in \\(X_i\\), \\(Y_i\\) is \\(e^{\\hat{\\beta}_1}\\) times more likely to be 1 than to be 0.\nCode example:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n# Load dataset\nfrom sklearn.datasets import load_iris\nX, y = load_iris(return_X_y=True)\nX, y = X[y != 2], y[y != 2]  # Binary classification\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Logistic Regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred = logreg.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\ndisp.plot(cmap='Blues', values_format='d')\n\nplt.title(\"Confusion Matrix for Logistic Regression\")\nplt.show()\n\nAccuracy: 1.0\n\n\n\n\n\n\n\n\n\nThe accuracy of 1.00 shows perfect classification on the test dataset, likely due to the dataset’s simplicity and separability.\nThe confusion matrix shows the counts of true positive, true negative, false positive, and false negative predictions."
  },
  {
    "objectID": "posts/post-with-code/index.html#comparing-ridge-and-logistic-regression",
    "href": "posts/post-with-code/index.html#comparing-ridge-and-logistic-regression",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "Comparing Ridge and Logistic Regression",
    "text": "Comparing Ridge and Logistic Regression\nWhile Ridge Regression is used for continuous outcomes, Logistic Regression addresses binary and multi-class classification tasks. Here are some insights into their comparison:\n\nMathematical foundation: Ridge regression minimizes the residual sum of squares with an L2 penalty, while logistic regression optimizes the log-likelihood function with regularization.\nScaling: both ridge regression and logistic regression typically benefit from scaling the input data, especially when using numerical features of varying ranges.\nOutput interpretability: Ridge regression provides straightforward coefficient estimates for continuous predictions, whereas logistic regression outputs probabilities for different classes.\n\n\n\n\n\n\n\n\n\nAspect\nRidge Regression\nLogistic Regression\n\n\n\n\nTask Type\nRegression (continuous)\nClassification (binary/multi-class)\n\n\nRegularization\nL2\nL1, L2, or Elastic Net\n\n\nOutput\nContinuous values\nProbabilities"
  },
  {
    "objectID": "posts/post-with-code/index.html#real-world-applications-of-regression",
    "href": "posts/post-with-code/index.html#real-world-applications-of-regression",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "Real-world Applications of Regression",
    "text": "Real-world Applications of Regression\nRidge regression helps us find out the relationships in data, and answer questions like: what’s the relationship between the amount of time you study and the exam scores. Many real-world cases could be solved by ridge regression:\n\nReal Estate: Estimating house prices by considering factors like location, size, and amenities.\nFinance: Forecasting stock prices and portfolio returns.\nMarketing: Predicting sales performance based on advertisement spend.\n\nIn real life, logistic regression is frequently used to forecast binary or multi-class outcomes, such as success/failure, win/lose. Some of the real-world cases are:\n\nHealthcare: Predicting the likelihood of heart disease.\nFinance: Detecting fraudulent transactions based on transaction patterns.\nChurn prediction: Predicting customer churn for subscription-based services."
  },
  {
    "objectID": "posts/post-with-code/index.html#tips-for-effective-regression-modeling",
    "href": "posts/post-with-code/index.html#tips-for-effective-regression-modeling",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "Tips for Effective Regression Modeling",
    "text": "Tips for Effective Regression Modeling\n\nImprove the data quality by removing outliers, and handling missing values.\nChoosing the optimal hyperparameters.\nDetecting multicollinearity in data by using metrics like Variance Inflation Factor (VIF).\nFeature Selection: Use techniques like PCA dimensionality reduction.\nInterpret Coefficients: Understand the impact of each feature on predictions."
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nRegression techniques like ridge and logistic regression are essential tools for data scientists. By mastering these models, you can tackle a wide range of prediction and classification problems in real life. Ridge regression helps in predicting continuous variables while handling overfitting through regularization. On the other hand, logistic regression helps to make decisions in binary and multi-class tasks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Mastering Regression: A Dive into Ridge and Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nData Science\n\n\nRegression\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nZhengling Jiang\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 14, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#references",
    "href": "posts/post-with-code/index.html#references",
    "title": "Mastering Regression: A Dive into Ridge and Logistic Regression",
    "section": "References",
    "text": "References\n\nScikit-learn Documentation"
  }
]